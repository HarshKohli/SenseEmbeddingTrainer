base_model: nli-roberta-base
crossencoder_base_model: microsoft/deberta-base
base_model_dir: models/base
batch_size: 32
train_dir: datasets/train
train_flat_file: train_gloss.csv
train_hyp_file: train_hyp.csv
train_combined_file: train_combined.csv
train_triplet_file: train_triplet.csv
eval_dir: datasets/eval
dev_file: semeval2007.csv
warmup_ratio: 0.1
test_files: [all.csv, semeval2007.csv, semeval2013.csv, semeval2015.csv, senseval2.csv, senseval3.csv]
saved_model_dir: model_save
num_epochs: 2
eval_steps: 10000
loss_type: OnlineContrastiveLoss
results_dir: results
use_hypernym: False
train_raw_dir: datasets/train/raw
train_raw_files: [semcor_train_sent_cls_ws.csv, glosses_main_train_sent_cls_ws.csv, examples_train_sent_cls_ws.csv]
oversample_ratio: 3
transfer_learn: False
eval_base: OnlineContrastiveLossLargeBatch
checkpoint_path: OnlineContrastiveLossLargeBatch
transfer_learn_base: ContrastiveLoss_hypernym
eval_strategy: Simple